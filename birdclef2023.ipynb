{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9dfeb2-c30d-4b25-9f49-c4fd95f66e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created credentials at: /root/.kaggle/kaggle.json\n",
      "üìÅ Download directory: /workspace/birdclef_2023\n",
      "‚¨áÔ∏è  Downloading BirdCLEF 2023 (this may take time)...\n",
      "Downloading birdclef-2023.zip to /workspace/birdclef_2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.91G/4.91G [00:21<00:00, 251MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Extracting birdclef-2023.zip ...\n",
      "‚úÖ All ZIPs extracted and removed.\n",
      "‚úÖ train_metadata.csv loaded: (16941, 12)\n",
      "  primary_label secondary_labels      type  latitude  longitude  \\\n",
      "0       abethr1               []  ['song']    4.3906    38.2788   \n",
      "1       abethr1               []  ['call']   -2.9524    38.2921   \n",
      "2       abethr1               []  ['song']   -2.9524    38.2921   \n",
      "\n",
      "      scientific_name               common_name         author  \\\n",
      "0  Turdus tephronotus  African Bare-eyed Thrush  Rolf A. de By   \n",
      "1  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n",
      "2  Turdus tephronotus  African Bare-eyed Thrush  James Bradley   \n",
      "\n",
      "                                             license  rating  \\\n",
      "0  Creative Commons Attribution-NonCommercial-Sha...     4.0   \n",
      "1  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n",
      "2  Creative Commons Attribution-NonCommercial-Sha...     3.5   \n",
      "\n",
      "                                 url              filename  \n",
      "0  https://www.xeno-canto.org/128013  abethr1/XC128013.ogg  \n",
      "1  https://www.xeno-canto.org/363501  abethr1/XC363501.ogg  \n",
      "2  https://www.xeno-canto.org/363502  abethr1/XC363502.ogg  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üê¶ BirdCLEF 2023 dataset download via Kaggle API (Method B)\n",
    "# Works on Windows / RunPod / Local Jupyter\n",
    "# ============================================================\n",
    "import os, sys, subprocess, glob, zipfile, pandas as pd, pathlib\n",
    "\n",
    "# 1Ô∏è‚É£  Fill in your Kaggle credentials here\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"vijaykalmani\"\n",
    "os.environ[\"KAGGLE_KEY\"]      = \"04ab696511da6beee0c5baf050229d57\"   # 64-character token, not your password\n",
    "\n",
    "# 2Ô∏è‚É£  Install Kaggle CLI if needed\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n",
    "\n",
    "# 3Ô∏è‚É£  Create ~/.kaggle/kaggle.json automatically\n",
    "home = pathlib.Path.home()\n",
    "kaggle_dir = home / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "(kaggle_dir / \"kaggle.json\").write_text(\n",
    "    f'{{\"username\":\"{os.environ[\"KAGGLE_USERNAME\"]}\",\"key\":\"{os.environ[\"KAGGLE_KEY\"]}\"}}'\n",
    ")\n",
    "print(f\"‚úÖ Created credentials at: {kaggle_dir / 'kaggle.json'}\")\n",
    "\n",
    "# 4Ô∏è‚É£  Choose where to save BirdCLEF data\n",
    "DEST = r\"C:\\birdclef_2023\" if os.name == \"nt\" else \"/workspace/birdclef_2023\"\n",
    "os.makedirs(DEST, exist_ok=True)\n",
    "print(f\"üìÅ Download directory: {DEST}\")\n",
    "\n",
    "# 5Ô∏è‚É£  Download all competition files\n",
    "print(\"‚¨áÔ∏è  Downloading BirdCLEF 2023 (this may take time)...\")\n",
    "subprocess.run(\n",
    "    [\"kaggle\", \"competitions\", \"download\", \"-c\", \"birdclef-2023\", \"-p\", DEST, \"--force\"],\n",
    "    check=True\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£  Unzip and clean up\n",
    "for zf in glob.glob(os.path.join(DEST, \"*.zip\")):\n",
    "    print(f\"üì¶ Extracting {os.path.basename(zf)} ...\")\n",
    "    with zipfile.ZipFile(zf, \"r\") as z:\n",
    "        z.extractall(DEST)\n",
    "    os.remove(zf)\n",
    "print(\"‚úÖ All ZIPs extracted and removed.\")\n",
    "\n",
    "# 7Ô∏è‚É£  Verify the data\n",
    "meta_path = os.path.join(DEST, \"train_metadata.csv\")\n",
    "if os.path.exists(meta_path):\n",
    "    df = pd.read_csv(meta_path)\n",
    "    print(\"‚úÖ train_metadata.csv loaded:\", df.shape)\n",
    "    print(df.head(3))\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  train_metadata.csv not found ‚Äî check that download succeeded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2fbff16-8ad0-4bc9-9f85-4fe6a1867e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] ROOT_DATA = /workspace/birdclef_2023\n",
      "[ENV] OUT_DIR   = /workspace/birdclef_2023/out\n",
      "[ENV] meta_csv  = /workspace/birdclef_2023/train_metadata.csv  | exists? True\n",
      "[ENV] audio_dir = /workspace/birdclef_2023/train_audio | exists? True\n",
      "[INFO] Using 15 species and 6227 files for this run.\n",
      "  primary_label             filename\n",
      "0        barswa  barswa/XC113914.ogg\n",
      "1        barswa  barswa/XC129647.ogg\n",
      "2        barswa  barswa/XC132406.ogg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6227/6227 [21:15<00:00,  4.88it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Dropping 72 malformed segments.\n",
      "[INFO] Saved features: 55062 | bad_paths: 0\n",
      "[INFO] Top species:\n",
      " species\n",
      "thrnig1    11614\n",
      "wlwwar      6025\n",
      "eubeat1     5596\n",
      "hoopoe      4446\n",
      "combuz1     4338\n",
      "cohmar1     3574\n",
      "barswa      3185\n",
      "eaywag1     2723\n",
      "comsan      2545\n",
      "combul2     2493\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Preprocessing complete. You can skip this cell next runs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1 ‚Äî Preprocessing: build & save log-mel segments\n",
    "# Produces:\n",
    "#   /workspace/birdclef_2023/out/features/X_logmel.npy  (object array of (128, SEG_FRAMES))\n",
    "#   /workspace/birdclef_2023/out/features/labels.csv    (columns: species, filename)\n",
    "# Run this only when you change preprocessing params or dataset scope.\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np, pandas as pd, librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (RunPod)\n",
    "# -------------------------\n",
    "ROOT_DATA = \"/workspace/birdclef_2023\"               # <‚Äî dataset root on RunPod\n",
    "OUT_DIR   = \"/workspace/birdclef_2023/out\"           # <‚Äî save outputs under the same tree\n",
    "os.makedirs(f\"{OUT_DIR}/features\", exist_ok=True)\n",
    "os.makedirs(f\"{OUT_DIR}/val_soundscapes\", exist_ok=True)\n",
    "\n",
    "meta_csv   = os.path.join(ROOT_DATA, \"train_metadata.csv\")\n",
    "audio_base = os.path.join(ROOT_DATA, \"train_audio\")\n",
    "\n",
    "assert os.path.isfile(meta_csv),  f\"Missing {meta_csv}\"\n",
    "assert os.path.isdir(audio_base), f\"Missing {audio_base}\"\n",
    "\n",
    "print(f\"[ENV] ROOT_DATA = {ROOT_DATA}\")\n",
    "print(f\"[ENV] OUT_DIR   = {OUT_DIR}\")\n",
    "print(f\"[ENV] meta_csv  = {meta_csv}  | exists? {os.path.isfile(meta_csv)}\")\n",
    "print(f\"[ENV] audio_dir = {audio_base} | exists? {os.path.isdir(audio_base)}\")\n",
    "\n",
    "# -------------------------\n",
    "# PREPROCESSING PARAMS\n",
    "# -------------------------\n",
    "SR, N_MELS, HOP = 32000, 128, 512\n",
    "SEG_DUR = 10.0\n",
    "SEG_FRAMES = int(SR * SEG_DUR / HOP)  # = 625 for 32k/512 over 10s\n",
    "N_SPECIES_QUICK = 15   # set to None to use full set\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def audio_to_logmel(path, sr=SR, n_mels=N_MELS, hop=HOP):\n",
    "    y, _ = librosa.load(path, sr=sr, mono=True, res_type=\"kaiser_fast\")\n",
    "    if y.size == 0: return None\n",
    "    m = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop, fmin=20, fmax=sr//2)\n",
    "    x = librosa.power_to_db(m, ref=np.max).astype(np.float32)\n",
    "    return np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "def segment_logmel(logmel, seg_frames=SEG_FRAMES, overlap=0.5, pad_short=True):\n",
    "    m = np.asarray(logmel, dtype=np.float32)\n",
    "    if m.ndim != 2: return\n",
    "    T = m.shape[1]\n",
    "    if T <= 0: return\n",
    "    if T < seg_frames:\n",
    "        if pad_short:\n",
    "            pad = seg_frames - T\n",
    "            pad_right = np.flip(m[:, max(0, T-pad):T], axis=1) if T > 0 else np.zeros((m.shape[0], pad), dtype=np.float32)\n",
    "            out = np.concatenate([m, pad_right], axis=1)[:, :seg_frames]\n",
    "            yield out\n",
    "        return\n",
    "    step = max(1, int(seg_frames * (1 - overlap)))\n",
    "    for s in range(0, T - seg_frames + 1, step):\n",
    "        e = s + seg_frames\n",
    "        yield m[:, s:e]\n",
    "\n",
    "def _fix_to_shape(seg, target_h=N_MELS, target_w=SEG_FRAMES):\n",
    "    \"\"\"Ensure segment is (target_h, target_w); pad/crop safely if needed.\"\"\"\n",
    "    if not isinstance(seg, np.ndarray) or seg.ndim != 2:\n",
    "        return None\n",
    "    seg = np.asarray(seg, dtype=np.float32)\n",
    "    h, w = seg.shape\n",
    "    # Fix freq dimension\n",
    "    if h != target_h:\n",
    "        if h > target_h:\n",
    "            seg = seg[:target_h, :]\n",
    "        else:\n",
    "            pad = np.zeros((target_h - h, w), dtype=np.float32)\n",
    "            seg = np.concatenate([seg, pad], axis=0)\n",
    "    # Fix time dimension\n",
    "    if w != target_w:\n",
    "        if w > target_w:\n",
    "            seg = seg[:, :target_w]\n",
    "        else:\n",
    "            need = target_w - w\n",
    "            ref = np.flip(seg[:, max(0, w-need):w], axis=1) if w > 0 else np.zeros((seg.shape[0], need), dtype=np.float32)\n",
    "            seg = np.concatenate([seg, ref], axis=1)[:, :target_w]\n",
    "    return np.nan_to_num(seg, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata & (optional) species filter\n",
    "# -------------------------\n",
    "meta = pd.read_csv(meta_csv)\n",
    "\n",
    "if N_SPECIES_QUICK is not None:\n",
    "    keep_species = meta[\"primary_label\"].value_counts().head(N_SPECIES_QUICK).index.tolist()\n",
    "    meta = meta[meta[\"primary_label\"].isin(keep_species)].reset_index(drop=True)\n",
    "\n",
    "print(f\"[INFO] Using {meta['primary_label'].nunique()} species and {len(meta)} files for this run.\")\n",
    "print(meta.head(3)[[\"primary_label\", \"filename\"]])\n",
    "\n",
    "# -------------------------\n",
    "# Build features (tracks filename per segment)\n",
    "# -------------------------\n",
    "X_feats, Y_labels, filenames = [], [], []\n",
    "bad_paths = 0\n",
    "\n",
    "for _, row in tqdm(meta.iterrows(), total=len(meta), desc=\"Preprocess\"):\n",
    "    sp = str(row[\"primary_label\"])\n",
    "    fn = str(row[\"filename\"])                  # e.g., \"barswa/XC113914.ogg\"\n",
    "    fpath = os.path.join(audio_base, fn)       # filename already includes subfolder\n",
    "    if not os.path.isfile(fpath):\n",
    "        bad_paths += 1\n",
    "        continue\n",
    "    lm = audio_to_logmel(fpath)\n",
    "    if lm is None:\n",
    "        continue\n",
    "    for seg in segment_logmel(lm, overlap=0.5, pad_short=True):\n",
    "        seg_fixed = _fix_to_shape(seg)\n",
    "        if seg_fixed is None:\n",
    "            continue\n",
    "        X_feats.append(seg_fixed)\n",
    "        Y_labels.append(sp)\n",
    "        filenames.append(fn)\n",
    "\n",
    "# Filter malformed (paranoia)\n",
    "ok = [i for i, s in enumerate(X_feats) if isinstance(s, np.ndarray) and s.shape == (N_MELS, SEG_FRAMES)]\n",
    "if len(ok) != len(X_feats):\n",
    "    dropped = len(X_feats) - len(ok)\n",
    "    print(f\"[WARN] Dropping {dropped} malformed segments.\")\n",
    "    X_feats   = [X_feats[i] for i in ok]\n",
    "    Y_labels  = [Y_labels[i] for i in ok]\n",
    "    filenames = [filenames[i] for i in ok]\n",
    "\n",
    "# -------------------------\n",
    "# Save features + labels (with filename)\n",
    "# -------------------------\n",
    "labels_df = pd.DataFrame({\"species\": Y_labels, \"filename\": filenames})\n",
    "\n",
    "# Save robustly as object array (avoids broadcasting)\n",
    "X_obj = np.empty(len(X_feats), dtype=object)\n",
    "for i, seg in enumerate(X_feats):\n",
    "    X_obj[i] = seg.astype(np.float32, copy=False)\n",
    "\n",
    "np.save(f\"{OUT_DIR}/features/X_logmel.npy\", X_obj)    # load later with allow_pickle=True\n",
    "labels_df.to_csv(f\"{OUT_DIR}/features/labels.csv\", index=False)\n",
    "\n",
    "print(f\"[INFO] Saved features: {len(X_feats)} | bad_paths: {bad_paths}\")\n",
    "print(\"[INFO] Top species:\\n\", labels_df[\"species\"].value_counts().head(10))\n",
    "print(\"‚úÖ Preprocessing complete. You can skip this cell next runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134b329-16e5-4107-9a5a-6165a8d3e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas librosa soundfile tqdm matplotlib scipy scikit-learn tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe95d560-cbf9-4610-9983-d55de5c37739",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas librosa soundfile tqdm matplotlib scipy scikit-learn \"tensorflow[and-cuda]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7886e8-93e8-4671-b80e-b766e7210573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-04 08:52:36.159938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MP] Using float32 for stability.\n",
      "[META] SR=32000 N_MELS=128 HOP=512 SEG_DUR=10.0 -> SEG_FRAMES=625\n",
      "[LOAD] Detected object array; reloading with allow_pickle=True and stacking...\n",
      "[CACHE] Loaded 55062 segments, 15 species.\n",
      "[CHECK] class counts: Counter({np.int64(12): 11614, np.int64(13): 6025, np.int64(7): 5596, np.int64(9): 4446, np.int64(4): 4338, np.int64(2): 3574, np.int64(0): 3185, np.int64(6): 2723, np.int64(5): 2545, np.int64(3): 2493, np.int64(14): 2133, np.int64(1): 2004, np.int64(11): 1904, np.int64(10): 1553, np.int64(8): 929})\n",
      "[SPLIT] Grouping by 'filename'.\n",
      "[SPLIT] Train=42911 Val=12151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759568237.460859    2080 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:41:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759568288.002735    2080 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-10-04 08:58:16.407892: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.4392 - loss: 7.6187[LR] epoch 1: 5e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 278ms/step - accuracy: 0.6197 - loss: 5.7348 - val_accuracy: 0.5402 - val_loss: 1.8414\n",
      "Epoch 2/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.6899 - loss: 5.9260[LR] epoch 2: 0.0001\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 265ms/step - accuracy: 0.7671 - loss: 4.7614 - val_accuracy: 0.6522 - val_loss: 1.4946\n",
      "Epoch 3/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.7374 - loss: 5.5047[LR] epoch 3: 0.00015\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.7968 - loss: 4.5161 - val_accuracy: 0.6943 - val_loss: 1.3926\n",
      "Epoch 4/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.7732 - loss: 5.2325[LR] epoch 4: 0.0002\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.8163 - loss: 4.3700 - val_accuracy: 0.7286 - val_loss: 1.3051\n",
      "Epoch 5/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.7775 - loss: 5.1748[LR] epoch 5: 0.000199619\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 260ms/step - accuracy: 0.8205 - loss: 4.3075 - val_accuracy: 0.7273 - val_loss: 1.3134\n",
      "Epoch 6/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.7992 - loss: 5.0163[LR] epoch 6: 0.000198481\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 261ms/step - accuracy: 0.8396 - loss: 4.1557 - val_accuracy: 0.6916 - val_loss: 1.3944\n",
      "Epoch 7/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8196 - loss: 4.8534[LR] epoch 7: 0.000196593\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 262ms/step - accuracy: 0.8529 - loss: 4.0429 - val_accuracy: 0.7487 - val_loss: 1.2707\n",
      "Epoch 8/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8333 - loss: 4.7391[LR] epoch 8: 0.000193969\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.8634 - loss: 3.9475 - val_accuracy: 0.7349 - val_loss: 1.3028\n",
      "Epoch 9/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8422 - loss: 4.6510[LR] epoch 9: 0.000190631\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 265ms/step - accuracy: 0.8699 - loss: 3.8874 - val_accuracy: 0.7753 - val_loss: 1.1741\n",
      "Epoch 10/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8569 - loss: 4.5474[LR] epoch 10: 0.000186603\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.8783 - loss: 3.8220 - val_accuracy: 0.7762 - val_loss: 1.1699\n",
      "Epoch 11/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8613 - loss: 4.4776[LR] epoch 11: 0.000181915\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 264ms/step - accuracy: 0.8828 - loss: 3.7501 - val_accuracy: 0.7989 - val_loss: 1.1379\n",
      "Epoch 12/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.8680 - loss: 4.4242[LR] epoch 12: 0.000176604\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 261ms/step - accuracy: 0.8907 - loss: 3.6959 - val_accuracy: 0.7812 - val_loss: 1.1524\n",
      "Epoch 13/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8794 - loss: 4.2833[LR] epoch 13: 0.000170711\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.8974 - loss: 3.6351 - val_accuracy: 0.7641 - val_loss: 1.2311\n",
      "Epoch 14/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.8823 - loss: 4.2855[LR] epoch 14: 0.000164279\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9006 - loss: 3.6185 - val_accuracy: 0.7352 - val_loss: 1.3208\n",
      "Epoch 15/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8817 - loss: 4.2883[LR] epoch 15: 0.000157358\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9035 - loss: 3.5637 - val_accuracy: 0.7610 - val_loss: 1.2724\n",
      "Epoch 16/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.8908 - loss: 4.2165[LR] epoch 16: 0.00015\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9078 - loss: 3.5454 - val_accuracy: 0.8048 - val_loss: 1.1334\n",
      "Epoch 17/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.8980 - loss: 4.1357[LR] epoch 17: 0.000142262\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9126 - loss: 3.4954 - val_accuracy: 0.8183 - val_loss: 1.0802\n",
      "Epoch 18/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9038 - loss: 4.1104[LR] epoch 18: 0.000134202\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 264ms/step - accuracy: 0.9153 - loss: 3.4768 - val_accuracy: 0.8292 - val_loss: 1.0449\n",
      "Epoch 19/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9034 - loss: 4.0468[LR] epoch 19: 0.000125882\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 261ms/step - accuracy: 0.9150 - loss: 3.4272 - val_accuracy: 0.8482 - val_loss: 0.9918\n",
      "Epoch 20/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.9046 - loss: 4.0056[LR] epoch 20: 0.000117365\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 265ms/step - accuracy: 0.9173 - loss: 3.4155 - val_accuracy: 0.8529 - val_loss: 0.9770\n",
      "Epoch 21/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.9084 - loss: 3.9848[LR] epoch 21: 0.000108716\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9176 - loss: 3.3907 - val_accuracy: 0.8272 - val_loss: 1.0396\n",
      "Epoch 22/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9129 - loss: 3.9227[LR] epoch 22: 0.0001\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9229 - loss: 3.3493 - val_accuracy: 0.8510 - val_loss: 0.9879\n",
      "Epoch 23/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9150 - loss: 3.9355[LR] epoch 23: 9.12844e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 264ms/step - accuracy: 0.9257 - loss: 3.3459 - val_accuracy: 0.8603 - val_loss: 0.9646\n",
      "Epoch 24/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9183 - loss: 3.8642[LR] epoch 24: 8.26352e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 263ms/step - accuracy: 0.9254 - loss: 3.3028 - val_accuracy: 0.8427 - val_loss: 1.0091\n",
      "Epoch 25/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9191 - loss: 3.8760[LR] epoch 25: 7.41181e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 261ms/step - accuracy: 0.9283 - loss: 3.3051 - val_accuracy: 0.8653 - val_loss: 0.9429\n",
      "Epoch 26/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.9211 - loss: 3.8742[LR] epoch 26: 6.5798e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9281 - loss: 3.3056 - val_accuracy: 0.8623 - val_loss: 0.9521\n",
      "Epoch 27/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9199 - loss: 3.8329[LR] epoch 27: 5.77382e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9276 - loss: 3.2823 - val_accuracy: 0.8617 - val_loss: 0.9535\n",
      "Epoch 28/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9240 - loss: 3.8445[LR] epoch 28: 5e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 263ms/step - accuracy: 0.9309 - loss: 3.2774 - val_accuracy: 0.8789 - val_loss: 0.9211\n",
      "Epoch 29/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9226 - loss: 3.8332[LR] epoch 29: 4.26423e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 260ms/step - accuracy: 0.9307 - loss: 3.2679 - val_accuracy: 0.8765 - val_loss: 0.9119\n",
      "Epoch 30/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9229 - loss: 3.8205[LR] epoch 30: 3.57212e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9318 - loss: 3.2446 - val_accuracy: 0.8812 - val_loss: 0.9032\n",
      "Epoch 31/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.9280 - loss: 3.7956[LR] epoch 31: 2.92893e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9333 - loss: 3.2472 - val_accuracy: 0.8804 - val_loss: 0.9063\n",
      "Epoch 32/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9289 - loss: 3.7700[LR] epoch 32: 2.33955e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9338 - loss: 3.2132 - val_accuracy: 0.8779 - val_loss: 0.9038\n",
      "Epoch 33/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9248 - loss: 3.8077[LR] epoch 33: 1.80848e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 263ms/step - accuracy: 0.9325 - loss: 3.2353 - val_accuracy: 0.8807 - val_loss: 0.8980\n",
      "Epoch 34/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9260 - loss: 3.8271[LR] epoch 34: 1.33975e-05\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9332 - loss: 3.2540 - val_accuracy: 0.8843 - val_loss: 0.8949\n",
      "Epoch 35/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9251 - loss: 3.8185[LR] epoch 35: 9.36922e-06\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 263ms/step - accuracy: 0.9315 - loss: 3.2484 - val_accuracy: 0.8893 - val_loss: 0.8823\n",
      "Epoch 36/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.9267 - loss: 3.8508[LR] epoch 36: 6.03073e-06\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 261ms/step - accuracy: 0.9329 - loss: 3.2713 - val_accuracy: 0.8890 - val_loss: 0.8823\n",
      "Epoch 37/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.9247 - loss: 3.8532[LR] epoch 37: 3.40741e-06\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 264ms/step - accuracy: 0.9320 - loss: 3.2842 - val_accuracy: 0.8921 - val_loss: 0.8758\n",
      "Epoch 38/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9202 - loss: 3.9519[LR] epoch 38: 1.51923e-06\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 263ms/step - accuracy: 0.9278 - loss: 3.3492 - val_accuracy: 0.8963 - val_loss: 0.8622\n",
      "Epoch 39/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.9235 - loss: 3.9594[LR] epoch 39: 3.80528e-07\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 264ms/step - accuracy: 0.9272 - loss: 3.3697 - val_accuracy: 0.9007 - val_loss: 0.8433\n",
      "Epoch 40/40\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.9277 - loss: 3.8987[LR] epoch 40: 0\n",
      "\u001b[1m670/670\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 262ms/step - accuracy: 0.9294 - loss: 3.3456 - val_accuracy: 0.9016 - val_loss: 0.8417\n",
      "[SAVED] model -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/model_crnn_final.keras\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      barswa     0.7783    0.8571    0.8158       602\n",
      "     blakit1     0.8193    0.8008    0.8100       487\n",
      "     cohmar1     0.8949    0.8760    0.8853       855\n",
      "     combul2     0.8786    0.8746    0.8766       654\n",
      "     combuz1     0.8766    0.9114    0.8937      1208\n",
      "      comsan     0.9143    0.9363    0.9252       581\n",
      "     eaywag1     0.8586    0.8953    0.8766       468\n",
      "     eubeat1     0.9892    0.9526    0.9705      1349\n",
      "      greegr     0.6398    0.7881    0.7062       151\n",
      "      hoopoe     0.9272    0.9483    0.9376       967\n",
      "      litegr     0.7975    0.8442    0.8202       308\n",
      "     rbsrob1     0.9537    0.8375    0.8918       443\n",
      "     thrnig1     0.9742    0.9174    0.9449      2178\n",
      "      wlwwar     0.9095    0.9131    0.9113      1508\n",
      "      woosan     0.7819    0.8597    0.8190       392\n",
      "\n",
      "    accuracy                         0.9016     12151\n",
      "   macro avg     0.8662    0.8808    0.8723     12151\n",
      "weighted avg     0.9048    0.9016    0.9025     12151\n",
      "\n",
      "[PATCH] Accuracy=90.16%  Macro-F1=0.8723\n",
      "[SAVED] Confusion matrix -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/val_confusion_top30.png\n",
      "\n",
      "===== üéß Clip-level (grouped by filename ) =====\n",
      "[CLIP][MAX ] Accuracy = 91.15% | Macro-F1 = 0.9029\n",
      "[CLIP][MEAN] Accuracy = 91.63% | Macro-F1 = 0.9084\n",
      "\n",
      "[CLIP][MAX ] classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      barswa     0.8632    0.8347    0.8487       121\n",
      "     blakit1     0.8750    0.8909    0.8829        55\n",
      "     cohmar1     0.8462    0.8462    0.8462        78\n",
      "     combul2     0.9138    0.8689    0.8908        61\n",
      "     combuz1     0.9712    0.9528    0.9619       106\n",
      "      comsan     0.9394    0.9490    0.9442        98\n",
      "     eaywag1     0.9608    0.9899    0.9751        99\n",
      "     eubeat1     0.9880    0.9762    0.9820        84\n",
      "      greegr     0.7027    0.7879    0.7429        33\n",
      "      hoopoe     0.9759    0.9529    0.9643        85\n",
      "      litegr     0.7941    0.8710    0.8308        62\n",
      "     rbsrob1     1.0000    0.8889    0.9412        54\n",
      "     thrnig1     0.9200    0.8762    0.8976       105\n",
      "      wlwwar     0.8713    0.9670    0.9167        91\n",
      "      woosan     0.9278    0.9091    0.9184        99\n",
      "\n",
      "    accuracy                         0.9115      1231\n",
      "   macro avg     0.9033    0.9041    0.9029      1231\n",
      "weighted avg     0.9135    0.9115    0.9119      1231\n",
      "\n",
      "\n",
      "[CLIP][MEAN] classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      barswa     0.8655    0.8512    0.8583       121\n",
      "     blakit1     0.8793    0.9273    0.9027        55\n",
      "     cohmar1     0.8500    0.8718    0.8608        78\n",
      "     combul2     0.9310    0.8852    0.9076        61\n",
      "     combuz1     0.9537    0.9717    0.9626       106\n",
      "      comsan     0.9300    0.9490    0.9394        98\n",
      "     eaywag1     0.9600    0.9697    0.9648        99\n",
      "     eubeat1     0.9880    0.9762    0.9820        84\n",
      "      greegr     0.7429    0.7879    0.7647        33\n",
      "      hoopoe     0.9639    0.9412    0.9524        85\n",
      "      litegr     0.8182    0.8710    0.8438        62\n",
      "     rbsrob1     1.0000    0.8519    0.9200        54\n",
      "     thrnig1     0.9231    0.9143    0.9187       105\n",
      "      wlwwar     0.8980    0.9670    0.9312        91\n",
      "      woosan     0.9462    0.8889    0.9167        99\n",
      "\n",
      "    accuracy                         0.9163      1231\n",
      "   macro avg     0.9100    0.9083    0.9084      1231\n",
      "weighted avg     0.9179    0.9163    0.9165      1231\n",
      "\n",
      "[SAVED] Clip-level results -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/val_clip_level_pooling.csv\n",
      "[SAVED] Clip conf (MAX) -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/clip_confusion_max.png\n",
      "[SAVED] Clip conf (MEAN) -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/clip_confusion_mean.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2 ‚Äî Train + Evaluate (CRNN+ full stack + clip-level)\n",
    "# ============================================================\n",
    "\n",
    "import os, random, warnings, glob, json, gc, shutil, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, average_precision_score,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# -------------------------\n",
    "# Paths (RunPod)\n",
    "# -------------------------\n",
    "# NOTE: Only paths changed to match your Cell 1 RunPod setup.\n",
    "OUT_DIR = \"/workspace/birdclef_2023/out\"\n",
    "FEAT_DIR = f\"{OUT_DIR}/features\"\n",
    "SCAPE_DIR = f\"{OUT_DIR}/val_soundscapes\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(SCAPE_DIR, exist_ok=True)\n",
    "\n",
    "CACHE_X = f\"{FEAT_DIR}/X_logmel.npy\"\n",
    "CACHE_Y = f\"{FEAT_DIR}/labels.csv\"\n",
    "META_JS = f\"{FEAT_DIR}/meta.json\"\n",
    "\n",
    "assert os.path.isfile(CACHE_X), f\"Missing {CACHE_X}\"\n",
    "assert os.path.isfile(CACHE_Y), f\"Missing {CACHE_Y}\"\n",
    "\n",
    "# -------------------------\n",
    "# Seed, GPU, Threads\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "for g in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    try: tf.config.experimental.set_memory_growth(g, True)\n",
    "    except: pass\n",
    "try:\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "except: pass\n",
    "\n",
    "mixed_precision.set_global_policy(\"float32\")\n",
    "print(\"[MP] Using float32 for stability.\")\n",
    "\n",
    "def float32_dense(units, **kw):\n",
    "    kw.setdefault(\"dtype\", \"float32\")\n",
    "    return layers.Dense(units, **kw)\n",
    "\n",
    "# -------------------------\n",
    "# Load metadata + cache\n",
    "# -------------------------\n",
    "with open(META_JS) as f:\n",
    "    meta = json.load(f)\n",
    "SR, N_MELS, HOP = int(meta[\"SR\"]), int(meta[\"N_MELS\"]), int(meta[\"HOP\"])\n",
    "SEG_DUR, SEG_FRAMES = float(meta[\"SEG_DUR\"]), int(meta[\"SEG_FRAMES\"])\n",
    "IMG_H, IMG_W = 224, 224\n",
    "print(f\"[META] SR={SR} N_MELS={N_MELS} HOP={HOP} SEG_DUR={SEG_DUR} -> SEG_FRAMES={SEG_FRAMES}\")\n",
    "\n",
    "labels_df = pd.read_csv(CACHE_Y)\n",
    "# OLD (replace these two lines)\n",
    "# X_LOGMEL = np.load(CACHE_X).astype(np.float32)\n",
    "# X_LOGMEL = np.nan_to_num(X_LOGMEL, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# NEW ‚Äî robust loader for object .npy caches\n",
    "def load_logmel(path, n_mels, seg_frames):\n",
    "    try:\n",
    "        arr = np.load(path, allow_pickle=False)\n",
    "    except ValueError as e:\n",
    "        if \"Object arrays cannot be loaded\" not in str(e):\n",
    "            raise\n",
    "        print(\"[LOAD] Detected object array; reloading with allow_pickle=True and stacking...\")\n",
    "        obj = np.load(path, allow_pickle=True)\n",
    "        fixed = []\n",
    "        for i, a in enumerate(obj):\n",
    "            a = np.asarray(a, dtype=np.float32).squeeze()\n",
    "            if a.ndim != 2:\n",
    "                raise ValueError(f\"Element {i} has shape {a.shape}, expected 2D.\")\n",
    "            # ensure mel dimension is n_mels\n",
    "            if a.shape[0] == n_mels:\n",
    "                pass\n",
    "            elif a.shape[1] == n_mels:\n",
    "                a = a.T\n",
    "            else:\n",
    "                raise ValueError(f\"Element {i} has mel dim {a.shape}, expected N_MELS={n_mels}.\")\n",
    "            # pad or center-crop time axis to seg_frames\n",
    "            t = a.shape[1]\n",
    "            if t < seg_frames:\n",
    "                pad = np.zeros((n_mels, seg_frames - t), dtype=np.float32)\n",
    "                a = np.concatenate([a, pad], axis=1)\n",
    "            elif t > seg_frames:\n",
    "                start = (t - seg_frames) // 2\n",
    "                a = a[:, start:start+seg_frames]\n",
    "            fixed.append(a)\n",
    "        arr = np.stack(fixed, axis=0)\n",
    "\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    return np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# use it here\n",
    "X_LOGMEL = load_logmel(CACHE_X, N_MELS, SEG_FRAMES)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels_df[\"species\"].astype(str))\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"[CACHE] Loaded {len(X_LOGMEL)} segments, {num_classes} species.\")\n",
    "print(\"[CHECK] class counts:\", Counter(y))\n",
    "\n",
    "# -------------------------\n",
    "# Grouped split (leakage-free)\n",
    "# -------------------------\n",
    "def _pick_groups(df):\n",
    "    cand = [\"rec_id\", \"recording_id\", \"filename\", \"file\", \"path\", \"filepath\"]\n",
    "    for c in cand:\n",
    "        if c in df.columns:\n",
    "            g = df[c].astype(str)\n",
    "            if c in [\"path\", \"filepath\"]:\n",
    "                g = g.apply(lambda p: os.path.basename(p))\n",
    "            return g.values, c\n",
    "    raise KeyError(\"No suitable group column found (need rec_id/filename).\")\n",
    "\n",
    "groups, group_col = _pick_groups(labels_df)\n",
    "print(f\"[SPLIT] Grouping by '{group_col}'.\")\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=SEED)\n",
    "tr_idx, val_idx = next(gss.split(X_LOGMEL, y, groups=groups))\n",
    "assert len(set(groups[tr_idx]).intersection(set(groups[val_idx]))) == 0, \"Group leakage!\"\n",
    "X_tr_raw, X_val_raw = X_LOGMEL[tr_idx], X_LOGMEL[val_idx]\n",
    "y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "print(f\"[SPLIT] Train={len(tr_idx)} Val={len(val_idx)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Run tag (avoid old shape restores)\n",
    "# -------------------------\n",
    "RUN_TAG = f\"crnnplus_H{IMG_H}W{IMG_W}_C{num_classes}_v5\"\n",
    "RUN_DIR = f\"{OUT_DIR}/{RUN_TAG}\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# tf.data pipeline\n",
    "# -------------------------\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def mel_to_img_tf(mel):\n",
    "    mel = tf.where(tf.math.is_finite(mel), mel, tf.zeros_like(mel))\n",
    "    vmin, vmax = tf.reduce_min(mel), tf.reduce_max(mel)\n",
    "    mel = (mel - vmin) / tf.maximum(vmax - vmin, 1e-6)\n",
    "    mel = tf.expand_dims(mel, -1)\n",
    "    mel = tf.image.resize(mel, (IMG_H, IMG_W))\n",
    "    mel = tf.tile(mel, [1, 1, 3])\n",
    "    return tf.cast(mel, tf.float32)\n",
    "\n",
    "def _map_fn(mel, label_oh):\n",
    "    return mel_to_img_tf(mel), tf.cast(label_oh, tf.float32)\n",
    "\n",
    "# MixUp WITHOUT tfp / tf.distributions\n",
    "def _beta_from_gamma(batch, alpha=0.4):\n",
    "    g1 = tf.random.gamma([batch], alpha, dtype=tf.float32)\n",
    "    g2 = tf.random.gamma([batch], alpha, dtype=tf.float32)\n",
    "    l = g1 / tf.maximum(g1 + g2, 1e-8)\n",
    "    return tf.reshape(l, [batch, 1, 1, 1])  # for images\n",
    "\n",
    "def mixup_batch(x, y, alpha=0.4):\n",
    "    b = tf.shape(x)[0]\n",
    "    l_img = _beta_from_gamma(b, alpha=alpha)\n",
    "    l_lbl = tf.reshape(l_img, [b, 1])\n",
    "    idx = tf.random.shuffle(tf.range(b))\n",
    "    xm = x * l_img + tf.gather(x, idx) * (1.0 - l_img)\n",
    "    ym = y * l_lbl + tf.gather(y, idx) * (1.0 - l_lbl)\n",
    "    return xm, ym\n",
    "\n",
    "def make_ds(X, y_oh, batch, shuffle=True, do_mixup=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y_oh))\n",
    "    if shuffle: ds = ds.shuffle(min(len(y_oh), 8192), seed=SEED)\n",
    "    ds = ds.map(_map_fn, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch, drop_remainder=do_mixup)\n",
    "    if do_mixup: ds = ds.map(lambda a,b: mixup_batch(a,b), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# -------------------------\n",
    "# One-hot + class weights\n",
    "# -------------------------\n",
    "y_tr_oh = tf.one_hot(y_tr, depth=num_classes)\n",
    "y_val_oh = tf.one_hot(y_val, depth=num_classes)\n",
    "cnt = Counter(y_tr.tolist())\n",
    "maxc = max(cnt.values())\n",
    "class_weights = {c: maxc/cnt[c] for c in cnt}\n",
    "\n",
    "# -------------------------\n",
    "# CRNN+ Model (Res + SE + MHSA)\n",
    "# -------------------------\n",
    "class SpecAugment(layers.Layer):\n",
    "    def __init__(self, fm=16, tm=32, nf=2, nt=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fm, self.tm, self.nf, self.nt = int(fm), int(tm), int(nf), int(nt)\n",
    "        self.active = False\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"fm\": self.fm, \"tm\": self.tm, \"nf\": self.nf, \"nt\": self.nt, \"active\": self.active})\n",
    "        return cfg\n",
    "    def call(self,x,training=False):\n",
    "        if not training or not self.active: return x\n",
    "        B,H,W,C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        def mask(axis_len, wmax, axis):\n",
    "            w = tf.random.uniform([], 0, tf.maximum(wmax, 1), dtype=tf.int32)\n",
    "            w = tf.minimum(w, axis_len-1)\n",
    "            s = tf.random.uniform([], 0, tf.maximum(axis_len-w,1), dtype=tf.int32)\n",
    "            if axis==1:\n",
    "                return tf.concat([tf.ones([B,s,W,C]), tf.zeros([B,w,W,C]), tf.ones([B,H-s-w,W,C])], axis=1)\n",
    "            else:\n",
    "                return tf.concat([tf.ones([B,H,s,C]), tf.zeros([B,H,w,C]), tf.ones([B,H,W-s-w,C])], axis=2)\n",
    "        y=x\n",
    "        for _ in range(self.nf): y = y*mask(H, self.fm, 1)\n",
    "        for _ in range(self.nt): y = y*mask(W, self.tm, 2)\n",
    "        return y\n",
    "\n",
    "def se_block(x,r=8):\n",
    "    c=x.shape[-1]\n",
    "    s=layers.GlobalAveragePooling2D()(x)\n",
    "    s=layers.Dense(max(c//r,4),activation=\"relu\")(s)\n",
    "    s=layers.Dense(c,activation=\"sigmoid\")(s)\n",
    "    return layers.Multiply()([x, layers.Reshape((1,1,c))(s)])\n",
    "\n",
    "def res_block(x,ch,pool=(2,2),drop=0.2):\n",
    "    h=layers.Conv2D(ch,3,padding=\"same\",use_bias=False)(x); h=layers.BatchNormalization()(h); h=layers.Activation(\"relu\")(h)\n",
    "    h=layers.Conv2D(ch,3,padding=\"same\",use_bias=False)(h); h=layers.BatchNormalization()(h)\n",
    "    if x.shape[-1]!=ch: x=layers.Conv2D(ch,1,padding=\"same\",use_bias=False)(x); x=layers.BatchNormalization()(x)\n",
    "    h=layers.Add()([x,h]); h=layers.Activation(\"relu\")(h); h=se_block(h)\n",
    "    h=layers.MaxPooling2D(pool)(h); h=layers.Dropout(drop)(h); return h\n",
    "\n",
    "def build_crnn(n_classes):\n",
    "    H3,W3,C3=IMG_H//8, IMG_W//4, 192\n",
    "    inp=keras.Input(shape=(IMG_H,IMG_W,3))\n",
    "    spec=SpecAugment(name=\"spec_augment\")\n",
    "    x=spec(inp)\n",
    "    x=res_block(x,64,(2,2),0.15)\n",
    "    x=res_block(x,128,(2,2),0.20)\n",
    "    x=res_block(x,192,(2,1),0.30)\n",
    "    x=layers.Permute((2,1,3))(x)      # (W3,H3,C3)\n",
    "    x=layers.Reshape((W3,H3*C3))(x)   # (56, 28*192)\n",
    "    x=layers.MultiHeadAttention(num_heads=4,key_dim=64,dropout=0.1)(x,x)\n",
    "    x=layers.LayerNormalization()(x)\n",
    "    x=layers.Bidirectional(layers.GRU(160,return_sequences=True))(x)\n",
    "    x=layers.Bidirectional(layers.GRU(160,return_sequences=True))(x)\n",
    "    a=layers.Dense(128,activation=\"tanh\")(x)\n",
    "    a=layers.Dense(1)(a)\n",
    "    a=layers.Softmax(axis=1)(a)\n",
    "    x=layers.Multiply()([x,a])\n",
    "    x=layers.Lambda(lambda t: keras.ops.sum(t,1))(x)\n",
    "    x=layers.Dropout(0.4)(x)\n",
    "    out=float32_dense(n_classes,activation=\"softmax\")(x)\n",
    "    return keras.Model(inp,out)\n",
    "\n",
    "model=build_crnn(num_classes)\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer & LR schedule (serializable)\n",
    "# -------------------------\n",
    "try:\n",
    "    register = keras.utils.register_keras_serializable\n",
    "except AttributeError:\n",
    "    from tensorflow.keras.utils import register_keras_serializable as register\n",
    "\n",
    "@register(package=\"custom\")\n",
    "class WarmupCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, warmup_steps, total_steps, name=\"WarmupCosine\"):\n",
    "        super().__init__()\n",
    "        self.base_lr = float(base_lr)\n",
    "        self.warmup_steps = int(warmup_steps)\n",
    "        self.total_steps = int(total_steps)\n",
    "        self.name = name\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warm = tf.cast(tf.maximum(1, self.warmup_steps), tf.float32)\n",
    "        total = tf.cast(tf.maximum(self.warmup_steps+1, self.total_steps), tf.float32)\n",
    "        warm_lr = self.base_lr * (step / warm)\n",
    "        pct = (step - warm) / tf.maximum(1.0, total - warm)\n",
    "        pct = tf.clip_by_value(pct, 0.0, 1.0)\n",
    "        cos_lr = 0.5 * self.base_lr * (1.0 + tf.cos(np.pi * pct))\n",
    "        return tf.where(step < warm, warm_lr, cos_lr)\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"base_lr\": self.base_lr,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"total_steps\": self.total_steps,\n",
    "            \"name\": self.name,\n",
    "        }\n",
    "\n",
    "batch_size = 64\n",
    "steps_per_epoch = max(1, len(y_tr)//batch_size)\n",
    "total_steps = steps_per_epoch * 40\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "sched = WarmupCosine(base_lr=2e-4, warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "opt = keras.optimizers.AdamW(learning_rate=sched, weight_decay=1e-5, clipnorm=1.0)\n",
    "loss = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "# -------------------------\n",
    "# Callbacks\n",
    "# -------------------------\n",
    "class ToggleSpecAug(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch>=2:\n",
    "            try: self.model.get_layer(\"spec_augment\").active=True\n",
    "            except: pass\n",
    "\n",
    "class LrLogger(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr_fn = self.model.optimizer.learning_rate\n",
    "        try:\n",
    "            lr_val = float(tf.keras.backend.get_value(lr_fn(self.model.optimizer.iterations)))\n",
    "        except TypeError:\n",
    "            lr_val = float(tf.keras.backend.get_value(lr_fn))\n",
    "        print(f\"[LR] epoch {epoch+1}: {lr_val:.6g}\")\n",
    "\n",
    "cb=[\n",
    "    keras.callbacks.BackupAndRestore(backup_dir=f\"{RUN_DIR}/backup_state\"),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",patience=6,restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(f\"{RUN_DIR}/best_crnn.keras\",monitor=\"val_accuracy\",save_best_only=True),\n",
    "    keras.callbacks.CSVLogger(f\"{RUN_DIR}/train_log.csv\",append=True),\n",
    "    ToggleSpecAug(),\n",
    "    LrLogger(),\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Training\n",
    "# -------------------------\n",
    "train_ds = make_ds(X_tr_raw, y_tr_oh, batch=batch_size, shuffle=True,  do_mixup=True)\n",
    "val_ds   = make_ds(X_val_raw, y_val_oh, batch=128,       shuffle=False, do_mixup=False)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=40,\n",
    "    callbacks=cb,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "keras.models.save_model(model, f\"{RUN_DIR}/model_crnn_final.keras\")\n",
    "print(\"[SAVED] model ->\", f\"{RUN_DIR}/model_crnn_final.keras\")\n",
    "\n",
    "# -------------------------\n",
    "# Eval (patch-level)\n",
    "# -------------------------\n",
    "y_val_prob = model.predict(val_ds, verbose=0)\n",
    "y_val_pred = y_val_prob.argmax(1)\n",
    "print(classification_report(y_val, y_val_pred, target_names=[le.classes_[k] for k in sorted(np.unique(y_val))], digits=4))\n",
    "print(f\"[PATCH] Accuracy={accuracy_score(y_val,y_val_pred)*100:.2f}%  Macro-F1={f1_score(y_val,y_val_pred,average='macro'):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Confusion matrix (top-30)\n",
    "# -------------------------\n",
    "top30 = [c for c,_ in Counter(y_val).most_common(30)]\n",
    "cm = confusion_matrix(y_val, y_val_pred, labels=top30, normalize='true')\n",
    "plt.figure(figsize=(10,10)); plt.imshow(cm, aspect='auto'); plt.title(\"Top-30 species ‚Äî normalized confusion\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.colorbar(); plt.tight_layout()\n",
    "plt.savefig(f\"{RUN_DIR}/val_confusion_top30.png\", dpi=150); plt.close()\n",
    "print(\"[SAVED] Confusion matrix ->\", f\"{RUN_DIR}/val_confusion_top30.png\")\n",
    "\n",
    "# =========================\n",
    "# Clip-level evaluation (pool by filename/recording)\n",
    "# =========================\n",
    "val_groups = np.asarray(groups)[val_idx]\n",
    "val_true   = y_val\n",
    "val_prob   = y_val_prob\n",
    "\n",
    "def group_mode(labels):\n",
    "    labels = np.asarray(labels, dtype=np.int64)\n",
    "    b = np.bincount(labels, minlength=num_classes)\n",
    "    return int(b.argmax())\n",
    "\n",
    "clip_rows = []\n",
    "for g in np.unique(val_groups):\n",
    "    sel = (val_groups == g)\n",
    "    probs_g = val_prob[sel]                 # (k, C)\n",
    "    labels_g = val_true[sel]                # (k,)\n",
    "    true_clip = group_mode(labels_g)\n",
    "    p_max  = probs_g.max(axis=0)\n",
    "    p_mean = probs_g.mean(axis=0)\n",
    "    pred_max  = int(p_max.argmax())\n",
    "    pred_mean = int(p_mean.argmax())\n",
    "    clip_rows.append({\n",
    "        \"group\": g, \"true\": true_clip,\n",
    "        \"pred_max\": pred_max, \"pred_mean\": pred_mean,\n",
    "        \"correct_max\": int(pred_max == true_clip),\n",
    "        \"correct_mean\": int(pred_mean == true_clip),\n",
    "        \"conf_max\": float(p_max[pred_max]),\n",
    "        \"conf_mean\": float(p_mean[pred_mean]),\n",
    "    })\n",
    "\n",
    "clip_df = pd.DataFrame(clip_rows)\n",
    "y_true_clip = clip_df[\"true\"].values\n",
    "y_pred_clip_max  = clip_df[\"pred_max\"].values\n",
    "y_pred_clip_mean = clip_df[\"pred_mean\"].values\n",
    "\n",
    "acc_clip_max  = accuracy_score(y_true_clip, y_pred_clip_max)\n",
    "acc_clip_mean = accuracy_score(y_true_clip, y_pred_clip_mean)\n",
    "f1_clip_max   = f1_score(y_true_clip, y_pred_clip_max, average=\"macro\")\n",
    "f1_clip_mean  = f1_score(y_true_clip, y_pred_clip_mean, average=\"macro\")\n",
    "\n",
    "print(\"\\n===== üéß Clip-level (grouped by\", group_col, \") =====\")\n",
    "print(f\"[CLIP][MAX ] Accuracy = {acc_clip_max*100:.2f}% | Macro-F1 = {f1_clip_max:.4f}\")\n",
    "print(f\"[CLIP][MEAN] Accuracy = {acc_clip_mean*100:.2f}% | Macro-F1 = {f1_clip_mean:.4f}\")\n",
    "\n",
    "tnames_all = list(le.classes_)\n",
    "print(\"\\n[CLIP][MAX ] classification report:\")\n",
    "print(classification_report(y_true_clip, y_pred_clip_max, target_names=tnames_all, digits=4))\n",
    "print(\"\\n[CLIP][MEAN] classification report:\")\n",
    "print(classification_report(y_true_clip, y_pred_clip_mean, target_names=tnames_all, digits=4))\n",
    "\n",
    "clip_out = clip_df.copy()\n",
    "clip_out[\"true_species\"] = [tnames_all[i] for i in clip_out[\"true\"].values]\n",
    "clip_out[\"pred_max_species\"]  = [tnames_all[i] for i in clip_out[\"pred_max\"].values]\n",
    "clip_out[\"pred_mean_species\"] = [tnames_all[i] for i in clip_out[\"pred_mean\"].values]\n",
    "clip_csv = f\"{RUN_DIR}/val_clip_level_pooling.csv\"\n",
    "clip_out.to_csv(clip_csv, index=False)\n",
    "print(\"[SAVED] Clip-level results ->\", clip_csv)\n",
    "\n",
    "cm_max = confusion_matrix(y_true_clip, y_pred_clip_max, normalize=\"true\")\n",
    "cm_mean = confusion_matrix(y_true_clip, y_pred_clip_mean, normalize=\"true\")\n",
    "plt.figure(figsize=(10,10)); plt.imshow(cm_max, aspect='auto'); plt.title(\"Clip-level (MAX) ‚Äî normalized confusion\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.colorbar(); plt.tight_layout()\n",
    "plt.savefig(f\"{RUN_DIR}/clip_confusion_max.png\", dpi=150); plt.close()\n",
    "print(\"[SAVED] Clip conf (MAX) ->\", f\"{RUN_DIR}/clip_confusion_max.png\")\n",
    "plt.figure(figsize=(10,10)); plt.imshow(cm_mean, aspect='auto'); plt.title(\"Clip-level (MEAN) ‚Äî normalized confusion\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.colorbar(); plt.tight_layout()\n",
    "plt.savefig(f\"{RUN_DIR}/clip_confusion_mean.png\", dpi=150); plt.close()\n",
    "print(\"[SAVED] Clip conf (MEAN) ->\", f\"{RUN_DIR}/clip_confusion_mean.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621b178d-bd55-40d8-8bd4-e858f84ba4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== üìä pcmAP ‚Äî Patch level =====\n",
      "[pcmAP][PATCH] macro (mean over classes) = 0.9237\n",
      "[pcmAP][PATCH] per-class CSV -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/pcmAP_patch_per_class.csv\n",
      "\n",
      "===== üéß pcmAP ‚Äî Clip level (grouped by filename) =====\n",
      "[pcmAP][CLIP][MEAN] macro = 0.9444  | per-class CSV -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/pcmAP_clip_mean_per_class.csv\n",
      "[pcmAP][CLIP][MAX ] macro = 0.8983  | per-class CSV -> /workspace/birdclef_2023/out/crnnplus_H224W224_C15_v5/pcmAP_clip_max_per_class.csv\n",
      "\n",
      "===== üìù Reviewer notes on pcmAP =====\n",
      "- We report *padded class-averaged mean average precision (pcmAP)* as the primary metric.\n",
      "- For each class we compute AP in a one-vs-rest setting on probabilities.\n",
      "- To avoid undefined cases for classes with no positives (or all positives) in a split, we apply conservative padding (append a zero-scored positive and/or a zero-scored negative) before AP.\n",
      "- pcmAP is then the macro-average of per-class APs. We provide results at both patch level and clip level (clip labels obtained by grouping on the leakage-free key and pooling segment probabilities by MEAN or MAX).\n",
      "- This addresses class imbalance and varying per-class support more robustly than accuracy; note our model previously achieves ~90% accuracy, and pcmAP complements that with calibrated ranking quality.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL ‚Äî pcmAP (padded class-averaged mean average precision)\n",
    "# ============================================================\n",
    "# Requirements: assumes you already have these in memory from Cell 2:\n",
    "#   RUN_DIR, le, num_classes, y_val (int class ids), y_val_prob (N_val x C),\n",
    "#   groups (full array from labels_df), val_idx, and group_col string.\n",
    "# If you only want patch-level pcmAP, you can ignore the clip section.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, f1_score\n",
    "import os\n",
    "\n",
    "assert 'y_val' in globals() and 'y_val_prob' in globals(), \"Run training/eval first to get y_val & y_val_prob.\"\n",
    "assert 'le' in globals() and 'num_classes' in globals(), \"Need label encoder and num_classes.\"\n",
    "assert 'RUN_DIR' in globals(), \"RUN_DIR not found.\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def _padded_ap(y_true_bin: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute AP with conservative 'padding' to avoid undefined cases:\n",
    "    - If a class has zero positives, append one positive with score 0 (and one negative with score 0).\n",
    "    - If a class has all positives, append one negative with score 0.\n",
    "    This keeps AP defined without inflating it.\n",
    "    \"\"\"\n",
    "    y_true = y_true_bin.astype(np.int8).ravel()\n",
    "    scores = y_score.astype(np.float32).ravel()\n",
    "    pos = y_true.sum()\n",
    "    if pos == 0:\n",
    "        y_true = np.concatenate([y_true, [1, 0]])\n",
    "        scores = np.concatenate([scores, [0.0, 0.0]])\n",
    "    elif pos == len(y_true):\n",
    "        y_true = np.concatenate([y_true, [0]])\n",
    "        scores = np.concatenate([scores, [0.0]])\n",
    "    return float(average_precision_score(y_true, scores))\n",
    "\n",
    "def compute_pcmAP(y_true_idx: np.ndarray, y_prob: np.ndarray, class_names=None):\n",
    "    \"\"\"\n",
    "    Patch-level pcmAP: per-class AP with padding, then macro average.\n",
    "    y_true_idx : (N,) integer class ids.\n",
    "    y_prob     : (N, C) probabilities (or scores) per class.\n",
    "    Returns: per_class_df, macro_pcmAP\n",
    "    \"\"\"\n",
    "    N, C = y_prob.shape\n",
    "    class_names = class_names or [f\"class_{i}\" for i in range(C)]\n",
    "    assert len(class_names) == C\n",
    "\n",
    "    # one-vs-rest ground truth\n",
    "    y_true_bin = np.eye(C, dtype=np.int8)[y_true_idx]  # (N, C)\n",
    "\n",
    "    aps, supports = [], []\n",
    "    for c in range(C):\n",
    "        aps.append(_padded_ap(y_true_bin[:, c], y_prob[:, c]))\n",
    "        supports.append(int(y_true_bin[:, c].sum()))\n",
    "    aps = np.array(aps, dtype=np.float32)\n",
    "    supports = np.array(supports, dtype=np.int32)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"class_idx\": np.arange(C),\n",
    "        \"class_name\": class_names,\n",
    "        \"support\": supports,\n",
    "        \"AP_padded\": aps\n",
    "    }).sort_values(\"class_idx\").reset_index(drop=True)\n",
    "\n",
    "    return df, float(np.mean(aps))\n",
    "\n",
    "def make_clip_level_probs(y_true_idx: np.ndarray,\n",
    "                          y_prob: np.ndarray,\n",
    "                          groups_array: np.ndarray,\n",
    "                          pooling: str = \"mean\"):\n",
    "    \"\"\"\n",
    "    Create clip-level (group-level) targets and probs using pooling.\n",
    "    y_true_idx  : (N,) int labels per segment\n",
    "    y_prob      : (N, C) probs per segment\n",
    "    groups_array: (N,) group key per segment (e.g., filename)\n",
    "    pooling     : 'mean' or 'max'\n",
    "    Returns: y_true_clip_idx (M,), y_prob_clip (M, C), unique_groups (M,)\n",
    "    \"\"\"\n",
    "    uniq = np.unique(groups_array)\n",
    "    M, C = len(uniq), y_prob.shape[1]\n",
    "    y_prob_clip = np.zeros((M, C), dtype=np.float32)\n",
    "    y_true_clip_idx = np.zeros((M,), dtype=np.int64)\n",
    "\n",
    "    # majority vote (mode) for clip's true label\n",
    "    def _mode1d(v):\n",
    "        v = np.asarray(v, dtype=np.int64)\n",
    "        b = np.bincount(v, minlength=C)\n",
    "        return int(b.argmax())\n",
    "\n",
    "    for i, g in enumerate(uniq):\n",
    "        sel = (groups_array == g)\n",
    "        P = y_prob[sel]  # (k, C)\n",
    "        if pooling == \"mean\":\n",
    "            pooled = P.mean(axis=0)\n",
    "        elif pooling == \"max\":\n",
    "            pooled = P.max(axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"pooling must be 'mean' or 'max'\")\n",
    "        y_prob_clip[i] = pooled\n",
    "        y_true_clip_idx[i] = _mode1d(y_true_idx[sel])\n",
    "\n",
    "    return y_true_clip_idx, y_prob_clip, uniq\n",
    "\n",
    "# -------------------------\n",
    "# Patch-level pcmAP\n",
    "# -------------------------\n",
    "tnames_all = list(le.classes_)\n",
    "\n",
    "patch_df, patch_pcmAP = compute_pcmAP(y_val, np.asarray(y_val_prob, dtype=np.float32), class_names=tnames_all)\n",
    "patch_csv = os.path.join(RUN_DIR, \"pcmAP_patch_per_class.csv\")\n",
    "patch_df.to_csv(patch_csv, index=False)\n",
    "\n",
    "print(\"\\n===== üìä pcmAP ‚Äî Patch level =====\")\n",
    "print(f\"[pcmAP][PATCH] macro (mean over classes) = {patch_pcmAP:.4f}\")\n",
    "print(f\"[pcmAP][PATCH] per-class CSV -> {patch_csv}\")\n",
    "\n",
    "# -------------------------\n",
    "# Clip-level pcmAP (MEAN & MAX pooling)\n",
    "# -------------------------\n",
    "assert 'groups' in globals() and 'val_idx' in globals() and 'group_col' in globals(), \\\n",
    "    \"Need groups, val_idx, group_col from the split.\"\n",
    "\n",
    "val_groups = np.asarray(groups)[val_idx]\n",
    "\n",
    "# MEAN pooling\n",
    "y_true_clip_mean, y_prob_clip_mean, uniq_groups_mean = make_clip_level_probs(\n",
    "    y_true_idx=y_val,\n",
    "    y_prob=np.asarray(y_val_prob, dtype=np.float32),\n",
    "    groups_array=val_groups,\n",
    "    pooling=\"mean\"\n",
    ")\n",
    "clip_mean_df, clip_mean_pcmAP = compute_pcmAP(y_true_clip_mean, y_prob_clip_mean, class_names=tnames_all)\n",
    "clip_mean_csv = os.path.join(RUN_DIR, \"pcmAP_clip_mean_per_class.csv\")\n",
    "clip_mean_df.to_csv(clip_mean_csv, index=False)\n",
    "\n",
    "# MAX pooling\n",
    "y_true_clip_max, y_prob_clip_max, uniq_groups_max = make_clip_level_probs(\n",
    "    y_true_idx=y_val,\n",
    "    y_prob=np.asarray(y_val_prob, dtype=np.float32),\n",
    "    groups_array=val_groups,\n",
    "    pooling=\"max\"\n",
    ")\n",
    "clip_max_df, clip_max_pcmAP = compute_pcmAP(y_true_clip_max, y_prob_clip_max, class_names=tnames_all)\n",
    "clip_max_csv = os.path.join(RUN_DIR, \"pcmAP_clip_max_per_class.csv\")\n",
    "clip_max_df.to_csv(clip_max_csv, index=False)\n",
    "\n",
    "print(f\"\\n===== üéß pcmAP ‚Äî Clip level (grouped by {group_col}) =====\")\n",
    "print(f\"[pcmAP][CLIP][MEAN] macro = {clip_mean_pcmAP:.4f}  | per-class CSV -> {clip_mean_csv}\")\n",
    "print(f\"[pcmAP][CLIP][MAX ] macro = {clip_max_pcmAP:.4f}  | per-class CSV -> {clip_max_csv}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08c11e-c2f7-4c6f-b51c-2a95cb3f6fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
